{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00a74cc",
   "metadata": {},
   "source": [
    "# Converting to an Open Neural Network Exchange (ONNX) Model\n",
    "\"ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.\"<br>\n",
    "[ONNX](https://onnx.ai)<br>\n",
    "<br>\n",
    "By converting our trained model to an ONNX format, we not only get a speed-up from memory and compute efficiencies, but we can port our model to other frameworks or use it with all sorts of Neural Network accelerators! This flexibility can be crucial for deployment or integration into different environments.<br>\n",
    "[How ONNX Optimizes Models](https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html#graph-optimizations-in-onnx-runtime)\n",
    "<br>\n",
    "<br>\n",
    "You can also just download a model from ONNX's model zoo!<br>\n",
    "[ONNX Model Zoo](https://onnx.ai/models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc488ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transform as defined for the pre-trained model\n",
    "# https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])]) \n",
    "\n",
    "device = torch.device(0 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained efficientnet_b1 model\n",
    "res_net = models.efficientnet_b1(weights=\"IMAGENET1K_V2\").to(device)\n",
    "\n",
    "# Set to eval mode for inference!\n",
    "res_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf71fe",
   "metadata": {},
   "source": [
    "## How fast is inference?\n",
    "Lets get a measure of how fast inference is with the Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test image\n",
    "test_image = Image.open(\"../../data/dog.jpg\")\n",
    "test_image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23804841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "tensor_image = transform(test_image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store inference times\n",
    "inference_time = []\n",
    "\n",
    "# Perform multiple inference runs (10 in this case)\n",
    "for _ in range(10):\n",
    "  # Record start time\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Forward pass of model\n",
    "  out_put = res_net(tensor_image.to(device))\n",
    "\n",
    "  # Record end time\n",
    "  end_time = time.time()\n",
    "\n",
    "  # Calculate and store inference time for this run\n",
    "  inference_time.append(end_time - start_time)\n",
    "\n",
    "# Print the minimum inference time observed across the runs\n",
    "print(\"Minimum inference time %.4fs\" % np.min(inference_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae85284",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n",
    "Pytorch has functionality to export a model to the ONNX format.<br>\n",
    "[About Pytorch ONNX](https://pytorch.org/docs/stable/onnx.html)<br>\n",
    "<br>\n",
    "We need to provide an example input that Pytorch will use to \"trace\" our model, basically \"recording\" the forward pass of our model in order to build the ONNX graph.<br>\n",
    "[Pytorch ONNX Tutorial](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)<br>\n",
    "<br>\n",
    "Because Pytorch needs to perform this tracing, we must make sure the forward pass only uses Pytorch functions and datatypes! It must also be deterministic, if your model's behaviour changes with conditional statements, then a trace won't be able to capture this!<br>\n",
    "[Avoiding ONNX Conversion Pitfalls](https://pytorch.org/docs/stable/onnx_torchscript.html#avoiding-pitfalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random input tensor to be used for tracing (it does not need to be a \"real\" example!)\n",
    "test_input = torch.randn(1, 3, 224, 224, device=device)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    res_net, # Model to convert\n",
    "    test_input, # Example input\n",
    "    \"efficientnet_b1.onnx\", # Output save name\n",
    "    opset_version=12, # Version of ONNX operations to use\n",
    "    export_params=True, # We will store the trained parameter weights inside the ONNX model file\n",
    "    do_constant_folding=True, # Whether to execute \"constant folding\" for optimization\n",
    "    input_names=['input'], # Define the model's input names\n",
    "    output_names=['output'], # Define the model's output names\n",
    "    dynamic_axes={'input' : {0 : 'batch_size'}, # Define any variable length axes\n",
    "                  'output' : {0 : 'batch_size'}}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
